---
title: "Variational Auto Encoder"
date: 2018-04-10
tags: [Computer vision]
excerpt: "Computer Vision"
mathjax: "true"
---
* Project description:
In this project, variational auto encoder(VAE) for MNIST dataset is used. The concept of VAE relies on the fact that an input image is mapped to a mean and variance of a normal distribution in the image latent space with low dimension. Then a sample is drown from the normal distribution and decoder reconstruct the original image from this sample of latent space. The reconstruction loss reflects the difference between the original and the decoded one while the regularization loss helps to learn the latent space and avoid overfitting on the training set.

* Encoder: It is comprised of three convolutional layers. After the output is falttened, it is fed to a fully connected layer. All activations are ReLU. The result is fed two different fully connected layers, one utilized to produce the mean and the other one is used to produce the variance of normal distribution. To sample from the normal distribution a function is introduced and since this is not a bulit-in layer in Keras, the following Python code is used:

```
z = layers.Lambda(sampling)([z_mean, z_log_var])
```     

* Decoder: Upsample the received sample of latent space and pass it through two convolutional layers to have an output of the same size as original image. The activation of first conv layer is ReLU while the second one has sigmoid activation.   

* The model is trained over 10 epochs.

* After training the model, decoder is  used to map the sampled points of latent space to images.

* The grid of decoded images are shown as follows:
<img src="{{ site.url }}{{ site.baseurl }}/images/VAE/mnist.png" alt="ooo">



The source code of this project is available in my [Github page](https://github.com/MohammadrezaAzimi/Cat-Dog-Classification-ConvNet/blob/master/Cats%20and%20dogs%20using%20Keras.ipynb). Interested reader is referred to the book *Deep Learning with Python* by Francois Chollet.         
