---
title: "Character-level text generation using LSTM"
date: 2018-04-07
tags: [Natural language processing]
excerpt: "NLP"
mathjax: "true"
---
* Project description:
In this project, the text corpus is the writing of German philosopher Nietzsche. An LSTM model is trained such that given N characters of the writers text, the next character will be generated by the model.

* Sequences of given length are extracted from the writing, one-hot encoded and described as 3D Numpy array of `(sequences, maxlen, unique_characters)` and then the target character of each array is also determined.    

* Network model: An LSTM layer with 128 cells is used and then the output is connected to a Dense layer with softmax activation.

* Sampling of the next character is done based on a multinomial distribution computed with given entropy  or temperature. The Python code is as follows:
```
def sample(preds, temperature):
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temperature
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    probas = np.random.multinomial(1, preds, 1)
    return np.argmax(probas)

```
* For 60 epochs, the model is fitted to the training data with batch sizes of 128. A text of length 400  is then generated for different values of temperature.

* The output for temperature 0.2 is
*new faculty, and the jubilation reached its climax when kant and such a man in the same time the spirit of the surely and the such the such
as a man is the sunligh and subject the present to the superiority of the special pain the most man and strange the subjection of the
special conscience the special and nature and such men the subjection of the
special men, the most surely the subjection of the special
intellect of the subjection of the same things and*

while the output for temperature 1 is

*cheerfulness, friendliness and kindness of a heart are spiritual by the
ciuture for the
entalled is, he astraged, or errors to our you idstood--and it needs,
to think by spars to whole the amvives of the newoatly, prefectly
raals! it was
name, for example but voludd atu-especity"--or rank onee, or even all
"solett increessic of the world and
implussional tragedy experience, transf, or insiderar,--must hast
if desires of the strubction is be stronges*

It can be seen that low temperature results in repetitive yet realistic structure while high temperature results in a different unforeseen structure.      

The source code of this project is available in my [Github link](https://github.com/MohammadrezaAzimi/TextGenerationLSTM-/blob/master/character%20level%20LSTM%20text%20generation%20(Language%20model).ipynb). Interested reader is referred to the book *Deep Learning with Python* by Francois Chollet.         
